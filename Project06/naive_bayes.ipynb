{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jonathan Sosa**\n",
    "\n",
    "Spring 2021\n",
    "\n",
    "CS 251: Data Analysis and Visualization\n",
    "\n",
    "Project 6: Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Preprocess full spam email dataset \n",
    "\n",
    "Before you build a Naive Bayes spam email classifier, run the full spam email dataset through your preprocessing code.\n",
    "\n",
    "Download and extract the full **Enron** emails (*zip file should be ~29MB large*). You should see a base `enron` folder, with `spam` and `ham` subfolders when you extract the zip file (these are the 2 classes).\n",
    "\n",
    "Run the test code below to check everything over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a) Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email_preprocessor as epp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `count_words` and `find_top_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq, num_emails = epp.count_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You found 32625 emails in the datset. You should have found 32625.\n"
     ]
    }
   ],
   "source": [
    "print(f'You found {num_emails} emails in the datset. You should have found 32625.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your top 5 words are\n",
      "['the', 'to', 'and', 'of', 'a']\n",
      "and they should be\n",
      "['the', 'to', 'and', 'of', 'a']\n",
      "The associated counts are\n",
      "[277459, 203659, 148873, 139578, 111796]\n",
      "and they should be\n",
      "[277459, 203659, 148873, 139578, 111796]\n"
     ]
    }
   ],
   "source": [
    "top_words, top_counts = epp.find_top_words(word_freq)\n",
    "print(f\"Your top 5 words are\\n{top_words[:5]}\\nand they should be\\n['the', 'to', 'and', 'of', 'a']\")\n",
    "print(f\"The associated counts are\\n{top_counts[:5]}\\nand they should be\\n[277459, 203659, 148873, 139578, 111796]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b) Make train and test splits of the dataset\n",
    "\n",
    "Here we divide the email features into a 80/20 train/test split (80% of data used to train the supervised learning model, 20% we withhold and use for testing / prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, y = epp.make_feature_vectors(top_words, num_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x_train, y_train, inds_train, x_test, y_test, inds_test = epp.make_train_test_sets(features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes for train/test splits:\n",
      "Train (26100, 200), classes (26100,)\n",
      "Test (6525, 200), classes (6525,)\n",
      "\n",
      "They should be:\n",
      "Train (26100, 200), classes (26100,)\n",
      "Test (6525, 200), classes (6525,)\n"
     ]
    }
   ],
   "source": [
    "print('Shapes for train/test splits:')\n",
    "print(f'Train {x_train.shape}, classes {y_train.shape}')\n",
    "print(f'Test {x_test.shape}, classes {y_test.shape}')\n",
    "print('\\nThey should be:\\nTrain (26100, 200), classes (26100,)\\nTest (6525, 200), classes (6525,)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c) Save data in binary format\n",
    "\n",
    "It adds a lot of overhead to have to run through your raw email -> train/test feature split every time you wanted to work on your project! In this step, you will export the data in memory to disk in a binary format. That way, you can quickly load all the data back into memory (directly in ndarray format) whenever you want to work with it again. No need to parse from text files!\n",
    "\n",
    "- Use numpy's `save` function to make six files in `.npy` format (e.g. `email_train_x.npy`, `email_train_y.npy`, `email_train_inds.npy`, `email_test_x.npy`, `email_test_y.npy`, `email_test_inds.npy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/email_train_x.npy', x_train)\n",
    "np.save('data/email_train_y.npy', y_train)\n",
    "np.save('data/email_train_inds.npy', inds_train)\n",
    "np.save('data/email_test_x.npy', x_test)\n",
    "np.save('data/email_test_y.npy', y_test)\n",
    "np.save('data/email_test_inds.npy', inds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Naive Bayes Classifier\n",
    "\n",
    "After finishing your email preprocessing pipeline, implement the one other supervised learning algorithm we we will use to classify email, **Naive Bayes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a) Implement Naive Bayes\n",
    "\n",
    "In `naive_bayes.py`, implement the following methods:\n",
    "- Constructor\n",
    "- `train(data, y)`: Train the Naive Bayes classifier so that it records the \"statistics\" of the training set: class priors (i.e. how likely an email is in the training set to be spam or ham?) and the class likelihoods (the probability of a word appearing in each class â€” spam or ham).\n",
    "- `predict(data)`: Combine the class likelihoods and priors to compute the posterior distribution. The predicted class for a test sample is the class that yields the highest posterior probability.\n",
    "- `accuracy(y, y_pred)`: The usual definition :)\n",
    "\n",
    "\n",
    "#### Bayes rule ingredients: Priors and likelihood (`train`)\n",
    "\n",
    "To compute class predictions (probability that a test example belong to either spam or ham classes), we need to evaluate **Bayes Rule**. This means computing the priors and likelihoods based on the training data.\n",
    "\n",
    "**Prior:** $$P_c = \\frac{N_c}{N}$$ where $P_c$ is the prior for class $c$ (spam or ham), $N_c$ is the number of training samples that belong to class $c$ and $N$ is the total number of training samples.\n",
    "\n",
    "**Likelihood:** $$L_{c,w} = \\frac{N_{c,w} + 1}{N_{c} + M}$$ where\n",
    "- $L_{c,w}$ is the likelihood that word $w$ belongs to class $c$ (*i.e. what we are solving for*)\n",
    "- $N_{c,w}$ is the total count of **word $w$** in emails that are only in class $c$ (*either spam or ham*)\n",
    "- $N_{c}$ is the total number of **all words** that appear in emails of the class $c$ (*total number of words in all spam emails or total number of words in all ham emails*)\n",
    "- $M$ is the number of features (*number of top words*).\n",
    "\n",
    "#### Bayes rule ingredients: Posterior (`predict`)\n",
    "\n",
    "To make predictions, we now combine the prior and likelihood to get the posterior:\n",
    "\n",
    "**Log Posterior:** $$Log(\\text{Post}_{i, c}) = Log(P_c) + \\sum_{j \\in J_i}x_{i,j}Log(L_{c,j})$$\n",
    "\n",
    " where\n",
    "- $\\text{Post}_{i,c}$ is the posterior for class $c$ for test sample $i$(*i.e. evidence that email $i$ is spam or ham*). We solve for its logarithm.\n",
    "- $Log(P_c)$ is the logarithm of the prior for class $c$.\n",
    "- $x_{i,j}$ is the number of times the jth word appears in the ith email.\n",
    "- $Log(L_{c,j})$: is the log-likelihood of the jth word in class $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from naive_bayes_multinomial import NaiveBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your class priors are: [0.24 0.26 0.25 0.25]\n",
      "and should be          [0.24 0.26 0.25 0.25].\n",
      "Your class likelihoods shape is (4, 6) and should be (4, 6).\n",
      "Your likelihoods are:\n",
      "[[0.15116 0.18497 0.17571 0.1463  0.16813 0.17374]\n",
      " [0.16695 0.17437 0.15742 0.16887 0.15677 0.17562]\n",
      " [0.14116 0.1562  0.19651 0.17046 0.17951 0.15617]\n",
      " [0.18677 0.18231 0.15884 0.12265 0.16755 0.18187]]\n",
      "and should be\n",
      "[[0.15116 0.18497 0.17571 0.1463  0.16813 0.17374]\n",
      " [0.16695 0.17437 0.15742 0.16887 0.15677 0.17562]\n",
      " [0.14116 0.1562  0.19651 0.17046 0.17951 0.15617]\n",
      " [0.18677 0.18231 0.15884 0.12265 0.16755 0.18187]]\n"
     ]
    }
   ],
   "source": [
    "num_test_classes = 4\n",
    "np.random.seed(0)\n",
    "data_test = np.random.random(size=(100, 6))\n",
    "y_test = np.random.randint(low=0, high=num_test_classes, size=(100,))\n",
    "\n",
    "nbc = NaiveBayes(num_classes=num_test_classes)\n",
    "nbc.train(data_test, y_test)\n",
    "\n",
    "print(f'Your class priors are: {nbc.class_priors}\\nand should be          [0.24 0.26 0.25 0.25].')\n",
    "print(f'Your class likelihoods shape is {nbc.class_likelihoods.shape} and should be (4, 6).')\n",
    "print(f'Your likelihoods are:\\n{nbc.class_likelihoods}')\n",
    "\n",
    "\n",
    "test_likelihoods = np.array([[0.15116, 0.18497, 0.17571, 0.1463 , 0.16813, 0.17374],\n",
    "       [0.16695, 0.17437, 0.15742, 0.16887, 0.15677, 0.17562],\n",
    "       [0.14116, 0.1562 , 0.19651, 0.17046, 0.17951, 0.15617],\n",
    "       [0.18677, 0.18231, 0.15884, 0.12265, 0.16755, 0.18187]])\n",
    "print(f'and should be\\n{test_likelihoods}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your predicted classes are\n",
      "[3 0 3 1 0 1 1 3 0 3 0 2 0 2 1]\n",
      "and should be\n",
      "[3 0 3 1 0 1 1 3 0 3 0 2 0 2 1]\n"
     ]
    }
   ],
   "source": [
    "num_test_classes = 4\n",
    "np.random.seed(0)\n",
    "data_train = np.random.randint(low=0, high=num_test_classes, size=(100, 10))\n",
    "data_test = np.random.randint(low=0, high=num_test_classes, size=(15, 10))\n",
    "y_test = np.random.randint(low=0, high=num_test_classes, size=(100,))\n",
    "\n",
    "nbc = NaiveBayes(num_classes=num_test_classes)\n",
    "nbc.train(data_train, y_test)\n",
    "test_y_pred = nbc.predict(data_test)\n",
    "\n",
    "print(f'Your predicted classes are\\n{test_y_pred}\\nand should be\\n[3 0 3 1 0 1 1 3 0 3 0 2 0 2 1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b) Spam filtering\n",
    "\n",
    "Let's start classifying spam email using the Naive Bayes classifier.\n",
    "\n",
    "- Use `np.load` to load in the train/test split that you created last week.\n",
    "- Use your Naive Bayes classifier on the Enron email dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7:** Print out the accuracy that you get on the test set with Naive Bayes. It should be roughly 89%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email_preprocessor as ep\n",
    "\n",
    "x_train = np.load('data/email_train_x.npy').astype('int')\n",
    "y_train = np.load('data/email_train_y.npy').astype('int')\n",
    "x_test = np.load('data/email_test_x.npy').astype('int')\n",
    "y_test = np.load('data/email_test_y.npy').astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.89\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes(num_classes = 2)\n",
    "nb.train(x_train, y_train)\n",
    "y_pred = nb.predict(x_test)\n",
    "accuracy = nb.accuracy(y_test, y_pred)\n",
    "\n",
    "print(f'accuracy is: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c) Confusion matrix\n",
    "\n",
    "To get a better sense of the errors that the Naive Bayes classifer makes, you will create a confusion matrix. \n",
    "\n",
    "- Implement `confusion_matrix` in `naive_bayes.py`.\n",
    "- Print out a confusion matrix of the spam classification results.\n",
    "\n",
    "**Debugging guidelines**:\n",
    "1. The sum of all numbers in your 2x2 confusion matrix should equal the number of test samples (6525).\n",
    "2. The sum of your spam row should equal the number of spam samples in the test set (3193)\n",
    "3. The sum of your ham row should equal the number of spam samples in the test set (3332)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3043.  150.]\n",
      " [ 552. 2780.]]\n"
     ]
    }
   ],
   "source": [
    "c = nb.confusion_matrix(y_test, y_pred)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8:** Interpret the confusion matrix, using the convention that positive detection means spam (*e.g. a false positive means classifying a ham email as spam*). What types of errors are made more frequently by the classifier? What does this mean (*i.e. X (spam/ham) is more likely to be classified than Y (spam/ham) than the other way around*)?\n",
    "\n",
    "**Reminder:** Look back and make sure you are clear on which class indices correspond to spam/ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 8:** tgere are more false negatives than false positives, causing that more spam is considered ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4d) Investigate the misclassification errors\n",
    "\n",
    "Numbers are nice, but they may not the best for developing your intuition. Sometimes, you want to see what an misclassification *actually looks like* to help you improve your algorithm. Here, you will take a false positive and a false negative misclassification and retrieve the actual text of the email so see which emails produced the error.\n",
    "\n",
    "- Determine the index of the **FIRST** false positive and false negative misclassification â€” i.e. 2 indices in total. Remember to use your `test_inds` array to look up the index of the emails BEFORE shuffling happened.\n",
    "- Implement the function `retrieve_emails` in `email_preprocessor.py` to return the string of the raw email at the error indices.\n",
    "- Call your function to print out the two emails that produced misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9:** Does it seem reasonable that each email message was misclassified? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 9:** \n",
    "\n",
    "yes, the false positive is an automated email from an antivirus company and the false negative is an advertisement with casual language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26954 12769]\n",
      "false positive\n",
      "Subject: re : mail authentification bad gateway : the message has been attached . + + + attachment : no virus found + + + panda antivirus - www . pandasoftware . com\n",
      "\n",
      "\n",
      "false negative\n",
      "Subject: schedule well , this season we lost mick and o ' neal and picked up daren and plove . i guess i ' ll be on the court more to utilize my quickness since we lost a couple of guards . we have a buy next week to open the season .\n"
     ]
    }
   ],
   "source": [
    "inds = np.where(y_pred != y_test)[0]\n",
    "\n",
    "falsePositive = inds_test[inds[np.in1d(inds, np.where(y_pred == 0)[0])][0]]\n",
    "falseNegative = inds_test[inds[np.in1d(inds, np.where(y_pred == 1)[0])][0]]\n",
    "#print(falsePositive)\n",
    "#print(falseNegative)\n",
    "\n",
    "i = np.array([falsePositive, falseNegative])\n",
    "emails = epp.retrieve_emails(i)\n",
    "print('false positive')\n",
    "print(emails[0])\n",
    "print(\"\\n\")\n",
    "print('false negative')\n",
    "print(emails[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Comparison with KNN\n",
    "\n",
    "\n",
    "- Run a similar analysis to what you did with Naive Bayes above. When computing accuracy on the test set, you may want to reduce the size of the test set (e.g. to the first 500 emails in the test set).\n",
    "- Copy-paste your `confusion_matrix` method into `knn.py` so that you can run the same analysis on a KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knn import KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurary: 0.911264367816092\n",
      "[[3130.   63.]\n",
      " [ 515. 2817.]]\n"
     ]
    }
   ],
   "source": [
    "knn = KNN(num_classes = 2)\n",
    "knn.train(x_train, y_train)\n",
    "y_predicted = knn.predict(x_test, 2)\n",
    "accuarcy = knn.accuracy(y_test, y_predicted)\n",
    "\n",
    "print(f'Accurary: {accuarcy}')\n",
    "print(knn.confusion_matrix(y_test, y_predicted.astype('int')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10:** What accuracy did you get on the test set (potentially reduced in size)?\n",
    "\n",
    "**Question 11:** How does the confusion matrix compare to that obtained by Naive Bayes (*If you reduced the test set size, keep that in mind*)?\n",
    "\n",
    "**Question 12:** Briefly describe at least one pro/con of KNN compared to Naive Bayes on this dataset.\n",
    "\n",
    "**Question 13:** When potentially reducing the size of the test set here, why is it important that we shuffled our train and test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 10:**.91\n",
    "\n",
    "**Answer 11:** KNN given the results of the confusion matrix\n",
    "\n",
    "**Answer 12:** KNN is slower but more acurate while Bayes is faster\n",
    "\n",
    "**Answer 13:** So that there is no bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Classify your own datasets\n",
    "\n",
    "- Find datasets that you find interesting and run classification on them using your KNN algorithm (and if applicable, Naive Bayes). Analysis the performance of your classifer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Better text preprocessing\n",
    "\n",
    "- If you look at the top words extracted from the email dataset, many of them are common \"stop words\" (e.g. a, the, to, etc.) that do not carry much meaning when it comes to differentiating between spam vs. non-spam email. Improve your preprocessing pipeline by building your top words without stop words. Analyze performance differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'is', 'enron', 'this', 'on']\n"
     ]
    }
   ],
   "source": [
    "word_freq_1, num_emails_1 = epp.count_words()\n",
    "top_words_1, top_counts_1 = epp.find_top_words(word_freq_1, num_features = 200)\n",
    "features_1, y_1 = epp.make_feature_vectors(top_words_1, num_emails_1)\n",
    "print(top_words_1[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this extension alters the emaol processos file and ignores common words, with the hope of increasing accuarcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.89\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "x_train1, y_train1, inds_train1, x_test1, y_test1, inds_test1 = epp.make_train_test_sets(features_1, y_1)\n",
    "\n",
    "x_train1 = x_train1.astype('int')\n",
    "y_train1 = y_train1.astype('int')\n",
    "x_test1 = x_test1.astype('int')\n",
    "y_test1 = y_test1.astype('int')\n",
    "inds_train1 = inds_train1.astype('int')\n",
    "inds_test1 = inds_test1.astype('int')\n",
    "\n",
    "nb1 = NaiveBayes(num_classes = 2)\n",
    "nb1.train(x_train1, y_train1)\n",
    "y_pred1 = nb1.predict(x_test1)\n",
    "accuracy = nb.accuracy(y_test1, y_pred1)\n",
    "\n",
    "print(f'accuracy is: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature size\n",
    "\n",
    "- Explore how the number of selected features for the email dataset influences accuracy and runtime performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Distance metrics\n",
    "- Compare KNN performance with the $L^2$ and $L^1$ distance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. K-Fold Cross-Validation\n",
    "\n",
    "- Research this technique and apply it to data and your KNN and/or Naive Bayes classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Email error analysis\n",
    "\n",
    "- Dive deeper into the properties of the emails that were misclassified (FP and/or FN) by Naive Bayes or KNN. What is their word composition? How many words were skipped because they were not in the training set? What could plausibly account for the misclassifications?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
